---
title: "Superagent Architecture"
description: "Token-efficient AI development via context injection vs. Multi-Agent duplication."
---

## Executive Summary

Membria introduces the **Superagent Architecture**, which transforms stateless AI agents (like Claude Code) into **decision-aware development companions** that leverage persistent context, saving up to 10x on token costs compared to conventional Multi-Agent systems.

**Core value proposition:**
- **For developers:** AI that remembers what worked and what didn't.
- **For teams:** Shared memory that survives personnel changes and context switches.
- **For budgets:** Architecture that minimizes LLM token consumption.

---

## The Problem: Stateless AI in Vibe Coding

### What is Vibe Coding
Vibe coding = describing what you want in natural language, letting AI write the code. It works great for one-off scripts and prototypes, but breaks down in long-term projects.

### Where It Breaks Down

| Situation | What Happens |
|-----------|--------------|
| Project lives > 1 week | AI doesn't remember yesterday's decisions |
| Multiple developers | No shared context between team members |
| Return to code after a month | "Why is it written this way?" — nobody knows |
| Similar problem solved before | AI makes the same mistake again |
| Bad approach tried previously | No memory → team tries it again |

### The Root Cause
Current AI coding assistants are **stateless**: every prompt is processed independently with no memory of past architectural decisions or "negative knowledge" (what failed).

---

## The Solution: Membria Superagent Architecture

### Instead of Multiple Agents (Multi-Agent)
Typical multi-agent systems (Orchestrator → Planner → Coder → Reviewer) burn **70,000+ tokens** per task because each agent receives full redundant context.

### Membria Approach: One Agent + Smart Context
```
✅ MEMBRIA SUPERAGENT:

User prompt
    ↓
┌─────────────────────────────────┐
│  Membria Graph Query            │  ← Local, 0 tokens
│  "What's relevant for this?"    │
└─────────────────────────────────┘
    ↓
[Only relevant context: ~2K tokens]
    ↓
┌─────────────────────────────────┐
│  Single Claude Code Call        │  ← 6-10K tokens total
│  With injected context          │
└─────────────────────────────────┘
    ↓
Code + Auto-captured decision record
```
**Result: 10x fewer tokens, persistent memory, predictable costs.**

---

## Architecture

```mermaid
flowchart TD
    subgraph "IDE / PR / CI Layer"
        A[VS Code, GitHub PR, Jira, CI jobs]
    end

    A --> CCP[Claude Control Plane (CCP)]

    subgraph CCP
        TR[Task Router]
        T1[Tactical Task]
        T2[Decision Task]
        T3[Learning Task]
        PG[Pre-Generation Context Fetch]
        DS[Decision Surface (IDE Inline)]
        DC[Decision Capture (DBB)]
        AG[Agent / TENN (graph + logic)]
    end

    CCP --> TR
    TR --> T1
    TR --> T2
    TR --> T3
    T2 --> PG
    PG --> DS
    DS --> DC
    DC --> AG

    AG --> PE[Policy Engine (Graph-Informed)]
    PE --> MCP[MCP Server (Claude Runtime Control)]
    MCP --> CC[Claude Code (stateless)]
    MCP --> VG[Post-Generation Validators]
    VG --> RG[Membria Reasoning Graph]
    RG --> OCL[Outcome Capture Layer]
    OCL --> RG
```

### Task Router Classification

- **code_gen** -> tactical (no decision capture)
- **architecture** -> decision (full Membria flow)
- **refactor** -> decision (if structural change)
- **debug** -> tactical (unless root cause choice)
- **library_choice** -> decision (always)

**Decision signals:**
- "choose", "decide", "should we", "which is better"
- Multiple alternatives mentioned
- Architectural keywords: "pattern", "structure", "design"

### Pre-Generation Context Fetch (Query Reasoning Graph)

- Past decisions on this module
- Failed similar assumptions
- Team calibration for this domain
- Negative knowledge alerts

### Decision Surface (inline in IDE)

Displays to developer:
- Past similar decisions
- Risk warnings
- Calibration hints
- Alternative suggestions

Buttons:
- [Proceed]
- [Review]
- [Override]

### Decision Capture (DBB)

Records:
- Statement
- Alternatives
- Confidence level
- Predicted outcome
- Context (immutable)

### Agent / TENN Execution Modes

- Tactical: direct code generation
- Decision: generate with captured context injected
- Learning: link outcome to historical decision

### Policy Engine (Graph-Informed)

Static rules:
- Security policies (no hardcoded secrets)
- Compliance requirements (GDPR, SOC2)
- Code style enforcement

Dynamic rules (from Reasoning Graph):
- Team-specific calibration adjustments
- Domain-specific confidence thresholds
- Negative knowledge enforcement

Resonance detection:
- Human bias + LLM bias alignment check
- Trigger friction if both ignore same risk

### MCP Server (Claude Runtime Control)

- Context injection:
  - Decision context from capture
  - Negative knowledge from Graph
  - Team preferences and patterns
- Tool exposure:
  - Code generation tools
  - Graph query tools
  - Decision recording tools
- Mode enforcement:
  - Tactical mode: minimal context
  - Decision mode: full context injection
- Output schema validation

### Post-Generation Validators

Bias detection in output:
- Anchoring: excessive focus on first option in code comments
- Confirmation: ignoring stated alternatives
- Overconfidence: "definitely", "always works", "no issues"

Consistency checks:
- Generated code matches captured decision?
- Negative knowledge respected?
- Alternatives actually considered in implementation?

Validation failures:
- Log for calibration (do not block)
- Alert if critical (security, compliance)

### Membria Reasoning Graph (Decision Record)

```
decision_id: dec_142
statement: "Use Fastify for REST API"
alternatives: ["Express.js", "Koa", "Custom"]
confidence: 0.60
predicted_outcome: "Stable API, good performance"
assumptions: ["Fastify handles our load", "Team knows it"]
context_hash: "abc123..." (immutable)
status: PENDING_OUTCOME
linked_pr: null -> PR#234 (when created)
linked_commit: null -> commit_sha (when merged)
```

Relationships:
- RELIES_ON: assumptions
- BLOCKS: dependent decisions
- SUPERSEDES: previous decisions on same topic
- CAUSED_BY: parent architectural decisions

### Outcome Capture Layer (Commitment Events)

| Source | Event | Outcome Signal |
|--------|-------|----------------|
| GitHub | PR merged | Decision executed |
| GitHub | PR closed (no merge) | Decision abandoned |
| CI/CD | Tests pass | Positive signal |
| CI/CD | Tests fail | Negative signal |
| CI/CD | Build fail | Negative signal |
| PagerDuty | Incident created | Strong negative |
| Jira | Bug linked to PR | Negative signal |
| Time | 30 days stable | Positive outcome |
| Time | 90 days stable | Strong positive outcome |

Calibration update:
- Compare predicted_outcome vs actual_outcome
- Update team calibration profile
- Update domain-specific calibration
- Generate LoRA candidate if systematic gap detected

## Vibe Coding Outcomes

### 1. Fewer "Generate → Break → Redo" Cycles
**Without Membria:** Claude generates custom middleware → PR fails security review → Rewrite → 2 days lost.
**With Membria:** Claude receives context *"Avoid custom auth middleware (Negative Knowledge)"* → Generates correct code using standard libraries (e.g., passport.js).
**Outcome: 60% reduction in rework time.**

### 2. Code Stays Understandable After a Month
Membria links code choices to specific Decision Records. You don't have to guess why Fastify was chosen over Express; the reason is in the Graph.
**Outcome: Self-documenting codebase.**

### 3. AI Stops Repeating Mistakes
When a bug is fixed, Membria records it as "Negative Knowledge." The next time Claude generates similar code, it receives context to avoid that specific pattern.
**Outcome: Project-specific learning.**

### 4. No More "Let's Try This" Without Consequences
Membria checks the Graph: "Tried this library 2 months ago? Yes. It failed due to X. **Don't repeat.**"
**Outcome: Team remembers what was already tried.**

### 5. Faster Onboarding
New developers instantly see **why** the architecture is the way it is by viewing the Decision Graph evolution, reducing context loss.
**Outcome: Onboarding time reduced by 50%.**

### 6. Confidence in Generated Code
Membria shows historical success/failure rates for similar patterns used by the team, increasing developer confidence in accepting generated code.

---

## Token Economics: Superagent vs Multi-Agent

### Per-Task Comparison

| Aspect | Multi-Agent System | Membria Superagent |
|--------|-------------------|--------------------|
| **Architecture** | N agents × full context | 1 agent × smart context |
| **Tokens per task** | 70,000+ | 6,000-10,000 |
| **Token growth** | O(n²) | O(1) |
| **Memory** | None (stateless) | Persistent (Graph) |
| **Cost Structure** | Usage-based, unpredictable | Subscription-based, fixed |

**Multi-agent vendors want you to use MORE agents. Membria wants you to use BETTER context to save tokens.**

---

## Bottom Line

Multi-agent architecture solves "how to divide work" by creating a new problem: context fragmentation. **Membria Superagent** solves the root problem: **giving one agent the right context.**

**Vibe coding stops being "generate from scratch every time" and becomes "generate with project memory.”**
---
